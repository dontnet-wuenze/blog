<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Flow-based Generative Model 学习笔记</title>
    <link href="/2022/0877c57936.html"/>
    <url>/2022/0877c57936.html</url>
    
    <content type="html"><![CDATA[<h1 id="Flow-based-Generative-Model"><a href="#Flow-based-Generative-Model" class="headerlink" title="Flow-based Generative Model"></a>Flow-based Generative Model</h1><p><a href="https://www.bilibili.com/video/av46561029/?p=59">李宏毅老师视频地址</a></p><p>参考了以下博客:</p><p><a href="http://www.gwylab.com/note-flow_based_model.html">【学习笔记】生成模型——流模型（Flow）</a></p><p><a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Flow-based Deep Generative Models</a></p><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>生成模型的统一目标都是对一个标准分布生成一个复杂分布, 使这个复杂分布和真实的分布尽量接近。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/generative.png" width="80%", height="80%"></div><p>Flow 模型可以直接优化这个目标式子</p><h2 id="数学补充"><a href="#数学补充" class="headerlink" title="数学补充"></a>数学补充</h2><h3 id="Jacobian-Matrix"><a href="#Jacobian-Matrix" class="headerlink" title="Jacobian Matrix"></a>Jacobian Matrix</h3><p>有两个向量&ensp; <script type="math/tex">x, z</script></p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/Jacoxz.png" width="40%", height="40%"></div><p>向量&ensp; <script type="math/tex">z</script>&ensp; 经过线性变换&ensp; <script type="math/tex">f</script>&ensp; 得到&ensp; <script type="math/tex">x</script>&ensp;</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/Jacoxt.png" width="20%", height="20%"></div><p>因此&ensp;<script type="math/tex">f</script>&ensp;的 Jacobian Matrix 为:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/Jacof.png" width="40%", height="40%"></div><p>举个例子:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jacosample1.png" width="30%", height="30%"></div><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jacosample2.png" width="20%", height="20%"></div><p>同理有&ensp;<script type="math/tex">z = f^{-1}(x)</script>&ensp;, 此时&ensp;<script type="math/tex">f^{-1}</script> &ensp;的 Jacobian Matrix 为:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jacozt.png" width="40%", height="40%"></div><p>有一个性质:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jacoxingzhi.png" width="20%", height="20%"></div><h3 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h3><p>行列式的计算方法<br>2 x 2:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/det22.png" width="40%", height="40%"></div><p>3 x 3:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/det33.png" width="40%", height="40%"></div><p>行列式的性质:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/detxingzhi.png" width="40%", height="40%"></div><p>&nbsp;<br>行列式的几何意义其实就是二维空间中的面积或三维空间中的体积:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/detxingzhi2.png" width="40%", height="40%"></div><h3 id="Change-Of-Variable-Theorem-变数变换"><a href="#Change-Of-Variable-Theorem-变数变换" class="headerlink" title="Change Of Variable Theorem(变数变换)"></a>Change Of Variable Theorem(变数变换)</h3><p>首先一个基本问题就是, 对于一个简单的分布&ensp;<script type="math/tex">\pi(z)</script>&ensp;和&ensp;<script type="math/tex">p(x)</script>&ensp;, 我们希望找到他们之间的变换函数&ensp;<script type="math/tex">f</script></p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/relation.png" width="80%", height="80%"></div><p>我们可以看一个简单的例子:</p><p>&ensp;<script type="math/tex">\pi(z)</script>&ensp;是一个 <script type="math/tex">[0, 1]</script> 上的均匀分布, 而&ensp;<script type="math/tex">p(x)</script>&ensp;是一个 <script type="math/tex">[1, 3]</script> 上的均匀分布.</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/z.png"></div><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/p.png"></div><p>&ensp;<script type="math/tex">z</script>&ensp;与&ensp; <script type="math/tex">x</script>&ensp;之间的变换函数其实就是: <script type="math/tex">x = f(x) = 2z + 1</script></p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/tran.png"></div><p>注意到&ensp;<script type="math/tex">x</script>&ensp;的底变宽了，为原来的 2 倍, 因此 x 的高其实是原来的 1/2。</p><p>现在我们的分布变成了一个复杂分布:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/complex.png" width="60%", height="60%"></div><p>对于一段极小的区间, <script type="math/tex">[z^{'}, z^{'} + \Delta z], [x^{'}, x^{'} + \Delta x]</script>&ensp;, 我们可以认为他们在这个区间里是均匀分布的。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/junyun.png" width="80%" height="80%"></div><p>而蓝色的方块和绿色的方块面积要相同，因此我们得到式子:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/shizi.png" width="40%", height="40%"></div><p>注意最后的式子需要绝对值, 因为&ensp;<script type="math/tex">x</script>&ensp;的变化可能是正, 也可能是负的。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/bianhua.png" width="60%", height="60%"></div><p>下面是二维的例子:</p><p>将一个&ensp;<script type="math/tex">z</script>&ensp;上小小的正方形投影到&ensp;<script type="math/tex">x</script>&ensp;上变成一个小小的菱形</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/erwei.png"></div><p>菱形的面积可以用行列式计算, 因此有:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/mianji.png" width="60%", height="60%"></div><p>上面的式子可以经过一系列的变换:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/tuidao.png" width="60%", height="60%"></div><p>最终得到:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jieguo.png" width="40%", height="40%"></div><h2 id="Flow-based-Model"><a href="#Flow-based-Model" class="headerlink" title="Flow-based Model"></a>Flow-based Model</h2><p>看看最初的目标:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/generative.png"></div><p>我们可以用&ensp;<script type="math/tex">z</script>&ensp;的分布写出&ensp;<script type="math/tex">x</script>&ensp;的分布了:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/zx.png" width="60%", height="60%"></div><p>要最大化这个式子有两个前提, 其一是&ensp;<script type="math/tex">det(J_{G})</script>&ensp;需要能够计算, 因为矩阵维数很大的话行列式的计算量很大, 其二是&ensp;<script type="math/tex">G^{-1}</script>&ensp; 需要知道, 这就意味着&ensp;<script type="math/tex">G^{-1}</script>&ensp;是可逆的, 因此&ensp;<script type="math/tex">z</script>&ensp;和&ensp;<script type="math/tex">x</script>&ensp;的维度也要是相同的。<br>从上面的分析可以发现, &ensp;<script type="math/tex">G</script>&ensp;的能力是很有限的, 因此我们可以通过流的方式来构建一个更加复杂的分布。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/flow.png"></div><p>对应的式子就是:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/liu.png" width="70%", height="70%"></div><h3 id="实际训练"><a href="#实际训练" class="headerlink" title="实际训练"></a>实际训练</h3><p>在实际的训练中, 我们是对&ensp;<script type="math/tex">G^{-1}</script>&ensp;进行训练, 训练好后再用&ensp;<script type="math/tex">G</script>&ensp;来生成。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/actual.png"></div><p>对应的最大化函数是:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/zuidahua.png" width="60%", height="60%"></div><p>注意到对于第一项, 因为&ensp;<script type="math/tex">z ~ \pi(z)</script>&ensp;, 因此&ensp;<script type="math/tex">z^{i}</script> 为 0 向量时最大。但是此时第二项会趋于 <script type="math/tex">-inf</script>。因此最大化是两项的均衡结果。</p><h2 id="几种-Flow-based-Model"><a href="#几种-Flow-based-Model" class="headerlink" title="几种 Flow-based Model"></a>几种 Flow-based Model</h2><h3 id="NICE-Real-NVP"><a href="#NICE-Real-NVP" class="headerlink" title="NICE/ Real NVP"></a>NICE/ Real NVP</h3><p><a href="https://arxiv.org/abs/1410.8516">NICE: Non-linear Independent Components Estimation</a><br><a href="https://arxiv.org/abs/1605.08803">Density estimation using Real NVP</a></p><p>这里采用了耦合层的设计(Coupling Layer)</p><h4 id="Coupling-Layer"><a href="#Coupling-Layer" class="headerlink" title="Coupling Layer"></a>Coupling Layer</h4><p>将&ensp;<script type="math/tex">z</script>&ensp;拆成两组, &ensp;<script type="math/tex">z_{1:d}</script>&ensp;一组, &ensp;<script type="math/tex">z_{d+1:D}</script>&ensp;一组。<br>&ensp;<script type="math/tex">z_{1:d}</script>&ensp;直接复制变成&ensp;<script type="math/tex">x_{1:d}</script>&ensp;<br>&ensp;<script type="math/tex">z_{1:d}</script>&ensp;经过一个函数&ensp;<script type="math/tex">F</script>&ensp;变成&ensp;<script type="math/tex">\beta_{d+1:D}</script>&ensp;, 经过一个函数&ensp;<script type="math/tex">H</script>&ensp;变成&ensp;<script type="math/tex">\gamma_{d+1:D}</script>&ensp;<br>然后有&ensp;<script type="math/tex">x_{i>d} = \beta_{i}z_{i} + \gamma_{i}</script>&ensp;<br>这里的&ensp;<script type="math/tex">F</script>, <script type="math/tex">H</script>&ensp;可以是任意的函数, 因为之后会发现&ensp;<script type="math/tex">G^{-1}</script>&ensp;和他们无关。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/couple.png" width="80%", height="80%"></div><p>因此, &ensp;<script type="math/tex">z</script>&ensp;变换成 &ensp;<script type="math/tex">x</script>&ensp;的式子可以写为:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/z2x.png"></div><p>逆变换的式子则有:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/x2z.png"></div><h4 id="Jacobian-行列式"><a href="#Jacobian-行列式" class="headerlink" title="Jacobian 行列式"></a>Jacobian 行列式</h4><p>Flow 的另一个关键在于 &ensp;<script type="math/tex">G</script>&ensp;的 &ensp;<script type="math/tex">Det(J)</script>&ensp;需要能够快速计算<br>事实上, &ensp;<script type="math/tex">J_{G}</script>&ensp; 就是下面的形式:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/JG.png" width="70%", height="70%"></div><p>左上角是直接复制过来的, 因此是单位矩阵。很明显右上角是 0, 因此计算行列式就与左下角无关了。 而右下角可以发现  &ensp;<script type="math/tex">x_{i>d}</script>&ensp; 只和  &ensp;<script type="math/tex">z_{i}</script>&ensp; 有关, 因此是对角矩阵。<br>所以有</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/JGshizi.png" width="50%", height="50%"></div><h4 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h4><p>因为我们的 Flow 是多层堆叠的, 所以我们的 Coupling layer 也要堆叠在一起。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/simple stack.png"></div>可以发现简单的堆叠前半部分就没有改变了,因此一种思想是我们的堆叠可以是交叉的。第一层复制前半部分,第二层复制后半部分。<div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/complexstack.png"></div>更具体的来说, 对于一个图片, 我们可以分为奇偶两种格子, 或者几个 channel 交叉。<div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/image.png" width="80%", height="80%"></div><h3 id="GLOW"><a href="#GLOW" class="headerlink" title="GLOW"></a>GLOW</h3><p><a href="https://arxiv.org/abs/1807.03039">Glow: Generative Flow with Invertible 1x1 Convolutions</a></p><h4 id="1x1-Convolution"><a href="#1x1-Convolution" class="headerlink" title="1x1 Convolution"></a>1x1 Convolution</h4><p>GLOW 引入了 W 矩阵来决定按什么顺序进行 copy 和 affine 变换，也就是 1x1 convolution。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/wmatrix.png" width="80%", height="80%"></div><p>这个 W 矩阵能够对通道进行交换。比如我们将 (1, 2, 3) 的向量替换成 (3, 1, 2) 的向量，只需将 W 矩阵定义如下。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jiaohuan.png" width="60%", height="60%"></div><p>下面就是 Glow 中一步 flow 的模型</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/one-glow-step.png" width="80%", height="80%"></div><h4 id="W-是否可逆"><a href="#W-是否可逆" class="headerlink" title="W 是否可逆?"></a>W 是否可逆?</h4><p>论文中并没有论述。文中的操作是初始化时用一个可逆矩阵, 之后矩阵很大概率也是可逆的。</p><h4 id="Jacobian-Determinant"><a href="#Jacobian-Determinant" class="headerlink" title="Jacobian Determinant"></a>Jacobian Determinant</h4><p>&ensp;<script type="math/tex">z</script>&ensp; 到 &ensp;<script type="math/tex">x</script>&ensp; 的变换为:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/wz.png" width="20%", height="20%"></div><p>因此&ensp;<script type="math/tex">f</script>&ensp;的 jacobian 其实就是&ensp;<script type="math/tex">W</script>&ensp;</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/real.png" width="40%", height="40%"></div><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/jf.png" width="60%", height="60%"></div><p>代入到含有 d 个 3 * 3 维的仿射变换矩阵当中, 得到最终的Jacobian行列式的计算结果就为&ensp;<script type="math/tex">(det(W))^{dxd}</script>&ensp;, 如下图所示:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Flow Base Generative Model/r11.png" width="80%", height="80%"></div><p>W 是 3x3 的矩阵, 因此计算就非常简单了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Flow</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VAE-GAN 结合 论文梳理</title>
    <link href="/2022/073dc06956.html"/>
    <url>/2022/073dc06956.html</url>
    
    <content type="html"><![CDATA[<h1 id="VAE-GAN-结合-论文梳理"><a href="#VAE-GAN-结合-论文梳理" class="headerlink" title="VAE-GAN 结合 论文梳理"></a>VAE-GAN 结合 论文梳理</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><h2 id="论文梳理"><a href="#论文梳理" class="headerlink" title="论文梳理"></a>论文梳理</h2><h3 id="VAE-GAN"><a href="#VAE-GAN" class="headerlink" title="VAE-GAN"></a>VAE-GAN</h3><p><a href="https://arxiv.org/abs/1512.09300">[2016][ICML]Autoencoding beyond pixels using a learned similarity metric-VAE-GANs</a></p><p>这篇论文模型的结构就是连接了 VAE 和 GAN。</p><div align="center" size="80%"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/VAE-GAN结合/VAE-GAN.png" width="80%", heigt="80%"></div><p>&emsp;</p><h4 id="不同视角"><a href="#不同视角" class="headerlink" title="不同视角"></a>不同视角</h4><ul><li><p>VAE视角 : 判别器具有 GAN 的性质, 生成图像更真实, 弥补 VAE 生成图像模糊的缺点。</p></li><li><p>GAN视角: 需要额外计算 VAE 的重构 loss, 提升了模型的稳定性。</p></li></ul><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>每个 iteration:</p><ul><li>从数据中采样 M 个图片 &nbsp;<script type="math/tex">x^{1}, x^{2}, ..., x^{M}</script></li><li>Encoder 生成 M 个隐变量 &nbsp;<script type="math/tex">\tilde{z}^{1}, \tilde{z}^{2}, ..., \tilde{z}^{M}</script><ul><li>&nbsp;<script type="math/tex">\tilde{z}^{i} = En(x^{i})</script></li></ul></li><li>Decoder 生成 M 个图片 &nbsp;<script type="math/tex">\tilde{x}^{1}, \tilde{x}^{2}, ..., \tilde{x}^{M}</script><ul><li>&nbsp;<script type="math/tex">\tilde{x}^{i} = De(\tilde{z}^{i})</script></li></ul></li><li>从先验分布&nbsp; <script type="math/tex">p(z)</script>&nbsp; 采样 M 个隐变量 &nbsp;<script type="math/tex">z^{1}, z^{2}, ..., z^{M}</script></li><li>Decoder 生成 M 个图片 &nbsp;<script type="math/tex">\hat{x}^{1}, \hat{x}^{2}, ..., \hat{x}^{M}</script><ul><li>&nbsp;<script type="math/tex">\hat{x}^{i} = De(z^{i})</script></li></ul></li><li><p>更新 Encoder 来降低降低生成 &nbsp; <script type="math/tex">\tilde{x}</script> &nbsp; 与 真实&nbsp; <script type="math/tex">x</script> &nbsp;差距&nbsp;<script type="math/tex">\left|| \tilde{x} - x|\right|</script>&nbsp;,<br>  即降低&nbsp; <script type="math/tex">KL(P(\tilde{z}|x)||p(z))</script>  </p></li><li><p>更新 Decoder 来降低降低生成 &nbsp; <script type="math/tex">\tilde{x}</script> &nbsp; 与 真实&nbsp; <script type="math/tex">x</script> &nbsp;差距&nbsp;<script type="math/tex">\left|| \tilde{x} - x|\right|</script>&nbsp;,<br>  同时提高&nbsp; <script type="math/tex">Dis(\tilde{x}) 和 Dis(\hat{x})</script>    </p></li><li>更新判别器来提高&nbsp; <script type="math/tex">Dis(x^{i})</script><br>  同时减小&nbsp; <script type="math/tex">Dis(\tilde{x}^{i})</script> 和 &nbsp; <script type="math/tex">Dis(\hat{x}^{i})</script></li></ul><h3 id="BiGANs"><a href="#BiGANs" class="headerlink" title="BiGANs"></a>BiGANs</h3><p><a href="https://arxiv.org/abs/1605.09782">[2017][ICLR] Adversarial Feature Learning-BiGANs</a></p><p><a href="https://arxiv.org/abs/1606.00704">[2017][ICLR]Adversarially Learned Inference-BiGANs</a></p><p>原始的 GAN 并不具备 Reconstruct 的能力, BiGAN 为 GAN 添加了 Encoder 的部分</p><div align="center" size="80%"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/VAE-GAN结合/BiGAN.png" width="80%", heigt="80%"></div><p>判别器的输入是原始图片和隐变量, 并判断这两个输入是来自 Encoder 还是 Decoder<br>理想情况下, BiGAN 就是一个 AutoEncoder, 此时 Encoder 和 Decoder 应该是互逆的, 即</p><div align="center" size="80%"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/VAE-GAN结合/optimal.png" width="50%", heigt="50%"></div><p>可是实践中并不能把这个模型训练到最优的情况, BiGAN 的重构效果也不是很好, 比如可能把一个鸟重构成另一只鸟。</p><h3 id="AAE"><a href="#AAE" class="headerlink" title="AAE"></a>AAE</h3><p><a href="https://arxiv.org/abs/1511.05644">[2016][ICLR]Adversarial Autoencoders-AAE</a></p><p>AAE 使用一个对抗网络来替代 VAE 中后验分布的 KL 散度</p><p>使&nbsp; <script type="math/tex">q(z|x) 和 p(x)</script> &nbsp;不断靠近</p><div align="center" size="80%"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/VAE-GAN结合/AAE.png" width="80%", heigt="80%"></div><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>相比 VAE 预设了 N(0,1) 的高斯分布, P(z)可以是任意的分布。</li></ul><h4 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h4><ul><li>Reconstruction: 训练 Encoder 和 Decoder</li><li>Regularization: 更新 D 和 Encoder(G)</li></ul>]]></content>
    
    
    <categories>
      
      <category>实验室</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>VAE</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Adversarial Network(GAN)学习笔记</title>
    <link href="/2022/07f4cb8fcb.html"/>
    <url>/2022/07f4cb8fcb.html</url>
    
    <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Network-GAN-学习笔记"><a href="#Generative-Adversarial-Network-GAN-学习笔记" class="headerlink" title="Generative Adversarial Network(GAN)学习笔记"></a>Generative Adversarial Network(GAN)学习笔记</h1><p>重度参考李宏毅老师的课程:</p><p><a href="https://www.youtube.com/watch?v=4OWp0wDu6Xw&amp;list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;index=14&amp;ab_channel=Hung-yiLee">https://www.youtube.com/watch?v=4OWp0wDu6Xw&amp;list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;index=14&amp;ab_channel=Hung-yiLee</a></p><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>从理解的角度来讲, GAN 由一个生成器和一个判别器组成。训练的过程就是生成器和判别器对抗的过程。判别器希望能分别出生成器生成的图片和真实的图片。而生成器则希望”以假乱真”, 使最终的判别器无法区分真实的图片和生成的图片。理想的情况下，最终的判别器具有很高的辨认能力, 而生成器生成的图片则几乎无法与真实图片区分。</p><div align="center" size="80%"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/zhiguanlijie.png" width="80%", heigt="80%"></div><p>&emsp;</p><p>比如一开始判别器会学习到真实图片都有眼睛, 这样生成器就会学习生成眼睛的能力, 之后判别器又会学习到真实的头像都有嘴巴, 生成器为了以假乱真也要学会生成嘴巴, 这就是一个对抗的过程, 最终使生成器具有生成能力。</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>训练过程很清晰, 首先固定生成器的参数, 对判别器进行训练。 之后固定判别器的参数, 对生成器进行训练。这样迭代多轮后直到训练结束。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/algorithm1.png" width="80%", heigt="80%"></div><h2 id="理论介绍"><a href="#理论介绍" class="headerlink" title="理论介绍"></a>理论介绍</h2><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>我们现在有一个生成器 G, 我们最终的目标就是希望 G 生成的分布和真实 Data 的分布是相似的。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/theoryimage.png" width="80%", heigt="80%"></div><p>&emsp;</p><p>写成公式就是下面的式子:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/theoryshizi.png" width="40%", heigt="70%"></div><h3 id="求解分布-Diversity"><a href="#求解分布-Diversity" class="headerlink" title="求解分布 Diversity"></a>求解分布 Diversity</h3><p>而问题在于, 我们有什么方法能求得这两个分布之间的 Diversity?</p><p>首先我们肯定没法直接去求分布的积分，因此可以采样。</p><p>一个比较直观的想法是用判别器去对采样的结果进行分类，如果分类准确率高就说明两者的分布相差大,好分辨。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/zhiguanbianbie.png" width="80%", heigt="80%"></div>          &emsp;公式化的写法就是我们要训练一个判别器, 而 V(G,D) 推导出来事实上就是两个分布间的 JS 散度。我们想要判别器的效果好, 就是要最大化 V。<div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/bianbieqi.png" width="80%", heigt="80%"></div><p>&emsp;</p><p>从另一个角度看, V 其实是交叉熵的负数, 因此我们要最大化 V, 也就是减小判别器的 Loss。</p><p>因为 V 越大，代表判别的准确度越高, 因此回到上面的式子, 最小化 Diversity, 也就是最小化 V。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/zuixiaohua.png" width="40%", heigt="80%"></div><h2 id="GAN-的缺陷"><a href="#GAN-的缺陷" class="headerlink" title="GAN 的缺陷"></a>GAN 的缺陷</h2><h3 id="JS-散度的局限性"><a href="#JS-散度的局限性" class="headerlink" title="JS 散度的局限性"></a>JS 散度的局限性</h3><p>可以看到, 我们在上面的公式中使用了 JS 散度来比较两个分布的相似度, 但这会带来问题, 因为 JS 散度没法评估两个不重合分布的相似度。</p><h3 id="为什么两个分布不重合"><a href="#为什么两个分布不重合" class="headerlink" title="为什么两个分布不重合"></a>为什么两个分布不重合</h3><h4 id="1-数据的性质"><a href="#1-数据的性质" class="headerlink" title="1. 数据的性质"></a>1. 数据的性质</h4><p>图片的分布在高维空间中可能是一条线, 或者只有很小一部分,这是因为大部分的空间中的点并不能组成一个完整的图像。因此两个分布只会有很少的部分相交, 可以忽略不记。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/fenbuxiangjiao.png" width="45%", heigt="45%"></div><h4 id="2-采样"><a href="#2-采样" class="headerlink" title="2. 采样"></a>2. 采样</h4><p>从分布中采样出的点是很少一部分, 即使整体分布有重合，采样的稀疏点也很容易被判别成两个不重合的分布。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/sample.png" width="45%", heigt="45%"></div><h3 id="JS-散度的问题"><a href="#JS-散度的问题" class="headerlink" title="JS 散度的问题"></a>JS 散度的问题</h3><p>只要两个分布不重合, JS 散度恒为 log2, 这就导致判别器没法训练。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/jsproblem.png" width="80%", heigt="80%"></div><h2 id="Wasserstein-distance-推土机距离"><a href="#Wasserstein-distance-推土机距离" class="headerlink" title="Wasserstein distance(推土机距离)"></a>Wasserstein distance(推土机距离)</h2><p>为了解决这个问题, 我们改用 Wasserstein distance 来判断两个分布的相似度。这个距离的定义就是把一个分布变成另一个分布需要移动的最小平均距离。</p><p>可以看到, 此时分布间的距离就会越来越近了。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/wdistance.png" width="80%", heigt="80%"></div><h2 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h2><h3 id="距离公式"><a href="#距离公式" class="headerlink" title="距离公式"></a>距离公式</h3><p>Wasserstein distance 也有对应的推导式子:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/wshizi.png" width="60%", heigt="80%"></div><p>&emsp;</p><p>可以看到, 这里对 D 有一个限制, 直白的说就是要求 D 是平滑的。</p><p>因为如果没有限制, D 就无法收敛了。(两个值会无限的增大和减小)</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/shoulian.png"width="60%", heigt="60%" ></div><h3 id="Lipschitz-Function"><a href="#Lipschitz-Function" class="headerlink" title="Lipschitz Function"></a>Lipschitz Function</h3><p>事实上 D 是要满足这个 Function 的性质, 也就是下面这个式子:</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/Lip.png"width="40%", heigt="40%" ></div><p>而 D 就是 K = 1 的情况。</p><h3 id="如何保证-D-是平滑的"><a href="#如何保证-D-是平滑的" class="headerlink" title="如何保证 D 是平滑的"></a>如何保证 D 是平滑的</h3><p>有几种方法使 D 拥有平滑的性质:</p><ul><li><p>Original WGAN → Weight</p><p>  可以限制 w 的值在 [-c, c] 之间, 这样相当于限制了 D 的上下界。</p></li><li><p>Improved WGAN → Gradient Penalty</p><p>  上面的 Lipschitz 性质有一个等价条件, 就是 D 对 x 的梯度都是小于等于 1 的</p>  <div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/gp.png"width="40%", heigt="40%" ></div><p>  很容易理解的方法就是对梯度加一个惩罚项。</p>  <div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/jifen1.png" width="40%", heigt="40%" ></div><p>  有一个问题在于我们没法对所有的X进行积分, 因此一个替代的方法是用 P<sub>data</sub> 和 P<sub>G</sub> 中间的 P<sub>penalty</sub> 的分布下的 X 作为惩罚项。</p>  <div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/ppen.png" width="60%", heigt="60%" ></div><p>  这样做看起来也确实是有一定道理的。 因为最终 P<sub>data</sub> 和 P<sub>G</sub> 逐渐靠近, 也只有他们中间的分布才会有影响。</p><p>  实验中有一个更好的收敛方法是梯度接近 1 是最好的。</p>  <div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/improve.png" width="60%", heigt="60%" ></div><p>  <a href="https://arxiv.org/abs/1704.00028">https://arxiv.org/abs/1704.00028</a></p></li><li><p>Spectral Normalization → Keep gradient norm smaller than 1 everywhere</p><p>  这个方法可以保证每个梯度都是小于 1 的</p><p>  <a href="https://arxiv.org/abs/1802.05957">https://arxiv.org/abs/1802.05957</a></p></li></ul><h3 id="WGAN-algorithm"><a href="#WGAN-algorithm" class="headerlink" title="WGAN algorithm"></a>WGAN algorithm</h3><p>WGAN 的算法在原始 GAN 的只需要四步改进:</p><ul><li><p>修改 D 的目标函数</p></li><li><p>D 的训练添加 Weight clipping / Gradient Penalty</p></li><li><p>修改 G 的目标函数</p></li><li><p>D 的输出不要 sigmoid 激活函数</p></li><li><p>优化器不要用 Adam, 用 SGD</p></li></ul><div align="center"><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Generative Adversarial Network(GAN)/wgan.png" width="90%", heigt="90%" ></div>]]></content>
    
    
    <categories>
      
      <category>实验室</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用yfinance获取美股数据的时报错</title>
    <link href="/2022/07ce066d14.html"/>
    <url>/2022/07ce066d14.html</url>
    
    <content type="html"><![CDATA[<p>报错信息:<br><code>No data found for this date range, symbol may be delisted</code></p><p>开始以为是股票信息出了问题，之后找到原因是大陆不能访问雅虎财经了，所以使用 yf.download时需要添加代理访问。</p><p>作者给出了添加代理的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> yfinance <span class="hljs-keyword">as</span> yf<br><br>msft = yf.Ticker(<span class="hljs-string">&quot;MSFT&quot;</span>)<br><br>msft.history(..., proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.get_actions(proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.get_dividends(proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.get_splits(proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.get_balance_sheet(proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.get_cashflow(proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br>msft.option_chain(..., proxy=<span class="hljs-string">&quot;PROXY_SERVER&quot;</span>)<br></code></pre></td></tr></table></figure><p>具体代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">stock_price = yf.download(<span class="hljs-string">&quot;AAPL&quot;</span>, start=<span class="hljs-string">&quot;2017-01-01&quot;</span>, end=<span class="hljs-string">&quot;2017-04-30&quot;</span>, proxy=<span class="hljs-string">&quot;http://127.0.0.1:7890&quot;</span>)<br></code></pre></td></tr></table></figure><p>这个代理地址是我的vpn的端口，问题解决</p>]]></content>
    
    
    <categories>
      
      <category>实验室</category>
      
    </categories>
    
    
    <tags>
      
      <tag>智能投顾</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VAE Auto-Encoding Variational Bayes 学习笔记</title>
    <link href="/2022/072abd261b.html"/>
    <url>/2022/072abd261b.html</url>
    
    <content type="html"><![CDATA[<h1 id="VAE-学习笔记"><a href="#VAE-学习笔记" class="headerlink" title="VAE 学习笔记"></a>VAE 学习笔记</h1><p>李宏毅老师的 <a href="https://www.bilibili.com/video/av15889450/?p=33">课程视频</a> 讲的很详细，记录一下</p><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>VAE 的大体结构和 Auto-Encoder 类似<br>区别在于 VAE 在生成的过程中加入了噪音, 使模型具有了生成能力</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/noise.jpg"></div><h2 id="VAE-的构造"><a href="#VAE-的构造" class="headerlink" title="VAE 的构造"></a>VAE 的构造</h2><p>从下图的结构可以看出，VAE 的 Encoder 会求出一个隐藏层的分布，然后对这个分布进行采样，生成一个 X’, 理想的情况下，这个 X’ 和 X 的距离应该是很小的</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/model.jpg"></div><p>但是问题在于，如果我们优化函数只优化最终结果的距离，那么X’最准确的情况的下分布的方差会变为 0, 此时的 VAE 事实上就退化成了 auto-encoder。 因此我们需要在 Loss 中加入对分布的限制, 从而达到较好的生成效果。</p><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>我们的目标是求解</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/maximum.png"></div><p>这个式子可以进行变换:</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/trans.png"></div><p>于是我们得到了一个 logP(x)的下界:</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Lb.png"></div><p>此时原式化为: </p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/yuanshi.png"></div><p>这里要补充一点，当我们在优化 Lb 的时候, KL散度是像0趋近的，这样我们logP(x)就会接近优化的下界。</p><p>注意到 </p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/lbtuidao.png"></div><p>因此只需要构造两个求解器，分别是 Encoder 和 Decoder</p><p>其中 Encoder 求解的是 q(z|x) 的分布，我们希望所有的 q(z|x) 都能近似 p(z) 的分布。z 的分布在 VAE 中定义成 N(0, 1)</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Encoder.jpg"></div><p>Decoder 求解的是 p(x|z) 的分布</p><div align=center><img src="https://cdn.jsdelivr.net/gh/dontnet-wuenze/picbed/Decoder.jpg"></div><p>至此证明完毕</p>]]></content>
    
    
    <categories>
      
      <category>实验室</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>VAE</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMU 149 lab1 Parallel Fractal Generation Using Threads</title>
    <link href="/2022/07655a184d.html"/>
    <url>/2022/07655a184d.html</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CMU149 lab0 环境搭建</title>
    <link href="/2022/07e401529b.html"/>
    <url>/2022/07e401529b.html</url>
    
    <content type="html"><![CDATA[<h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>附上官方地址:</p><p><a href="https://github.com/stanford-cs149/asst1">https://github.com/stanford-cs149/asst1</a></p><h2 id="安装-ISPC"><a href="#安装-ISPC" class="headerlink" title="安装 ISPC"></a>安装 ISPC</h2><p>首先需要安装 ISPC</p><p>我这里是 win11 使用 wsl 虚拟机<br>如果虚拟机或者 linux 系统一样的操作</p><p><code>wget https://github.com/ispc/ispc/releases/download/v1.16.1/ispc-v1.16.1-linux.tar.gz</code></p><p>下载后解压</p><p><code>tar -xvf ispc-v1.16.1-linux.tar.gz</code></p><h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><p><code>export PATH=$PATH:$&#123;HOME&#125;/ispc-v1.16.1-linux/bin</code></p><p>如果只在命令行里输入的话是临时的，下次就没有了<br>所以我们最好添加永久的环境变量</p><p>bash 用户可以添加到 ~/.bashrc 文件中</p><p>我这里在 zsh 添加了永久的环境变量</p><p>添加后输入</p><p><code>source $HOME/.zshrc</code></p><p>使环境变量生效</p><p>如果 lab1 的样例程序编译通过就代表安装成功了</p><p><a href="http://bossalex.top/2022/07/10/lab1-Parallel-Fractal-Generation-Using-Threads/">lab1 Parallel Fractal Generation Using Threads</a></p>]]></content>
    
    
    <categories>
      
      <category>课程学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CMU 149</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客迁移公告</title>
    <link href="/2022/07404cc7c6.html"/>
    <url>/2022/07404cc7c6.html</url>
    
    <content type="html"><![CDATA[<p>Hello 大家好, 欢迎来到大财主的代码乐园</p><p>之前的博客记录可以访问</p><p><a href="https://www.cnblogs.com/wuenze/">https://www.cnblogs.com/wuenze/</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
